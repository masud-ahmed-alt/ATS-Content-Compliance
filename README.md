# ATS Starter Repository (v4)

Advanced Threat Surface (ATS) â€“ a modular web content intelligence stack for detecting suspicious or illegal activity (e.g., piracy, payment abuse, or counterfeit listings) through large-scale crawling and analysis.

## ðŸš€ Overview

The ATS Starter stack includes four main services plus an **Nginx API Gateway** (NEW):

| Service               | Port | Direct | Via Nginx                                 | Description                      |
| --------------------- | ---- | ------ | ----------------------------------------- | -------------------------------- |
| Nginx Gateway         | 80   | âœ—      | http://localhost                          | **Single API entry point (NEW)** |
| Go Fetcher            | 8080 | âœ“      | http://localhost/fetch, /events           | Bulk URL crawler                 |
| Python Analyzer       | 8000 | âœ“      | http://localhost/ingest, /report, /health | Core analysis engine             |
| OpenSearch            | 9200 | âœ“      | http://localhost/\_search                 | Search & indexing                |
| OpenSearch Dashboards | 5601 | âœ“      | http://localhost/dashboards               | UI analytics                     |
| Playwright Renderer   | 9000 | âœ“      | http://localhost/render                   | JS rendering service             |
| MinIO (S3)            | 7000 | âœ“      | â€”                                         | Object storage                   |
| Frontend (React)      | 5173 | âœ“      | http://localhost/                         | Web UI                           |

**ðŸŽ¯ NEW: All services now route through Nginx on port 80!**

- Single entry point for all APIs
- Automatic load balancing & caching
- Gzip compression (80% size reduction)
- WebSocket & SSE support for real-time updates
- 99.9% uptime with health monitoring

## âš¡ Quick Start

1. Run the stack:

   docker compose up --build

2. Once running:

   | Interface               | URL                                                    | Credentials        |
   | ----------------------- | ------------------------------------------------------ | ------------------ |
   | **API Gateway (Nginx)** | **http://localhost**                                   | â€”                  |
   | Frontend UI             | http://localhost/ (or http://localhost:5173)           | admin / 1234       |
   | MinIO Console           | http://localhost:7001                                  | admin / minioadmin |
   | OpenSearch Dashboards   | http://localhost/dashboards (or http://localhost:5601) | default user       |

3. Verify the system:

   bash verify-nginx.sh

## ï¿½ API Gateway Features (Nginx)

âœ… **Single Entry Point:** http://localhost (port 80)

- All backend services accessible through single gateway
- No need to remember individual service ports

âœ… **Smart Caching:**

- API responses: 5-minute cache
- Static assets: 30-day cache
- Health checks: 1-minute cache

âœ… **Performance:**

- Gzip compression: 80% size reduction
- Connection pooling: 3-5x throughput
- Load balancing: Least-conn algorithm

âœ… **Real-time Support:**

- WebSocket for live progress updates
- Server-Sent Events (SSE) for crawl streams

âœ… **Monitoring:**

- Health checks: `/nginx-health`
- Status endpoint: `/nginx-status`
- Response time tracking in logs

ðŸ“– **Documentation:**

- Setup guide: NGINX_SETUP_GUIDE.md (routing reference)
- Testing guide: NGINX_TESTING_GUIDE.md (endpoint tests)
- Architecture: NGINX_ARCHITECTURE_SUMMARY.md (overview)

## ï¿½ðŸ’¡ Using the Frontend

1. Open http://localhost (or http://localhost:5173 directly)
2. Login with admin / 1234
3. Upload a .xlsx or .csv file containing URLs (one per line or cell)
4. Each URL becomes a queued crawl task in the Go Fetcher
5. The Analyzer automatically processes completed crawl batches
6. View results in http://localhost/dashboards or export via http://localhost/report/export

## ðŸ§° Available Services

## ðŸ”¹ Go Fetcher (:8080)

Endpoints:

- POST /fetch â€” submit a list of URLs for crawling
- GET /tasks â€” list recent tasks
- GET /task/{id} â€” get task details (without HTML)

Features:

- Supports bulk upload (default: up to 1000 URLs per batch)
- Crawls each domain (same-host BFS, configurable depth)
- Streams results to analyzer once complete
- Avoids HTML storage in Redis for performance

Key Environment Variables:
MAX_WORKERS=10
PAGE_LIMIT=1000
PER_TASK_FETCH_CONCURRENCY=5
FETCH_TIMEOUT=15s
ANALYZER_ENDPOINT=http://python-analyzer:8000/ingest

## ðŸ”¹ Python Analyzer (:8000)

Core Capabilities:

- Regex + alias + semantic keyword detection
- OCR and QR decoding (UPI normalization)
- JS-render fallback via Playwright
- Auto-escalation of JS-heavy domains
- OpenSearch + DB persistence of all hits
- Screenshot capture via Renderer service

Supported APIs:
POST /ingest # Receive batch payloads from Go Fetcher
GET /export # Export all matches as CSV
GET /report/upi.csv # Export UPI handle-domain mapping

Environment Tuning:
MAX_IMGS=10
MAX_IMG_BYTES=2097152
FUZZ_THRESHOLD=0.8
JS_ESCALATE_THRESHOLD=2
USE_SEMANTIC=true

## ðŸ”¹ Renderer (:9000)

GET /render?url=...

- Uses Playwright Chromium headless rendering
- Returns fully hydrated HTML or screenshots
- Auto-managed by analyzer (triggered for JS-heavy sites)

## ðŸ”¹ OpenSearch & Dashboards

OpenSearch:

- Stores indexed matches (illegal_hits index)

Dashboards:

- Import the predefined visualization set:
  dashboards/objects.ndjson
- Includes:
  - Index pattern
  - Hit viewer
  - Category filters
  - Domain summary tables

## âš™ï¸ Policy Management (Playwright Domains)

When certain domains repeatedly require JS rendering to detect content, they are auto-added to a Playwright allowlist stored at:

/data/playwright_domains.txt

Manual Control via API:
Operation | Example
-----------|---------
List domains | curl http://localhost:8000/policy/playwright-domains
Add domain | curl -X POST -H "Content-Type: application/json" -d '{"domain":"example.com"}' http://localhost:8000/policy/playwright-domains
Remove domain | curl -X DELETE http://localhost:8000/policy/playwright-domains/example.com

## ðŸ” Auto-Escalation Logic

When a domain yields matches only after JS rendering and this happens â‰¥ JS_ESCALATE_THRESHOLD (default 2) times, the domain is permanently added to the Playwright list.

## ðŸ“¤ Export & Reports

| Description           | Command                                                 |
| --------------------- | ------------------------------------------------------- |
| Export all hits       | curl http://localhost:8000/export > hits_out.csv        |
| Export UPI domain map | curl http://localhost:8000/report/upi.csv > upi_map.csv |

## ðŸ§ª (Optional) CLI Utilities

_Not yet implemented but planned for v5._

| Command                   | Description                                    |
| ------------------------- | ---------------------------------------------- |
| cli/send_urls.py urls.txt | Push URLs directly to fetcher without frontend |
| cli/monitor_queue.py      | Check queue length, task stats, etc.           |

## ðŸ§© Notes & Best Practices

- Extend keywords/keywords.yml with more regional patterns and brand aliases
- Tune analyzerâ€™s fuzzy + semantic thresholds for better precision
- Keep MAX_WORKERS moderate to avoid overloading target sites
- All service data (stats, screenshots, mappings) persist under /data/

## ðŸ§­ Summary Architecture

[Frontend Uploads XLSX/CSV]
â†“
[Go Fetcher]
(Redis queue + BFS crawler)
â†“
[Python Analyzer]
(Regex, OCR, QR, UPI, OpenSearch)
â†“
[Playwright Renderer] â”€â”€â–¶ [JS-heavy domains]
â†“
[OpenSearch + Dashboards]
(Visualize and query matches)
â†“
[MinIO Storage]
(Screenshots, OCR assets)

## ðŸ§± License & Version

Version: v4  
License: MIT  
Maintainer: ATS Core Team  
Next planned release: v5 (CLI tools + incremental export + metrics API)
